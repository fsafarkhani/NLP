{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18tDHlR1762LG1sTzDA9oVZhRioYfxsrR",
      "authorship_tag": "ABX9TyMM3e/cOjxEerL7Ma9qyhJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fsafarkhani/NLP/blob/main/NLP_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction"
      ],
      "metadata": {
        "id": "6746sfzrzI8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should process some texts using [NLTK](https://www.nltk.org/) or [spaCy](https://spacy.io/) libraries (ideally both). In particular, you should do the following:\n",
        "- Load the `harry_potter` book. You can find this text corpus in the datasets folder.\n",
        "- Segment the text of the book into sentences. How many sentences does this book have?\n",
        "- Compute the frequency of each token in the book. What are the most frequent tokens?\n",
        "- Choose a sentence from the book. Analyze this chosen sentence by\n",
        "    - Calculating all [n-grams](https://en.wikipedia.org/wiki/N-gram).\n",
        "    - Finding [POS tags](https://en.wikipedia.org/wiki/Part-of-speech_tagging) of tokens.\n",
        "    - [Stemming](https://en.wikipedia.org/wiki/Stemming) and [lemmatizing](https://en.wikipedia.org/wiki/Lemmatisation) tokens.\n",
        "- Check the documentation to identify the most important hyperparameters, attributes, and methods. Use them in practice."
      ],
      "metadata": {
        "id": "PxJgS3nFSOO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries:\n",
        "\n",
        " - nltk: Natural Language Toolkit (basic NLP tasks like tokenization, tagging, stemming).\n",
        "\n",
        " - spacy: Advanced NLP library for fast sentence parsing, POS tagging, etc.\n",
        "\n",
        " - textacy: Built on top of spaCy, helps with advanced tasks like n-gram extraction.\n",
        "\n",
        " - nlp = spacy.load(\"en_core_web_sm\"): Loads the English model into nlp."
      ],
      "metadata": {
        "id": "88RPMPPWzK9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "xQqerVjUQprz"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import textacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the File\n",
        "Opens the file and reads the full Harry Potter text into the variable text."
      ],
      "metadata": {
        "id": "Rdag3byTzUsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f= open(\"/content/harry_potter.txt\")\n",
        "text = f.read()\n",
        "print(text [:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQfiHDpnzZwW",
        "outputId": "a677bef4-0d71-43e7-ee7f-c31526ed720c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER ONE THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \n",
            "\n",
            "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \n",
            "\n",
            "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley's sister, b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment the text into Sentencce"
      ],
      "metadata": {
        "id": "GhpYNr2N0qE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.sent_tokenize: Splits the text into sentences using NLTK andcuts sentences based on punctuation, so it might break sentences"
      ],
      "metadata": {
        "id": "JnEBliCPSqj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk_Sentences = nltk.sent_tokenize(text)\n",
        "len(nltk_Sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QlfZpxx1Cy3",
        "outputId": "84129cbb-2d10-4d7f-8103-1025d7712168"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6394"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nlp(text).sents: spaCy also does sentence segmentation, but its more accure, because spaCy understands the grammar better, so it gives cleaner, more accurate sentence splits."
      ],
      "metadata": {
        "id": "VFIrKnoASvgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "spacy_sentences = list(doc.sents)\n",
        "len(spacy_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNKTZqr00tsu",
        "outputId": "c57b198a-fb40-4416-eae6-8b64315d6bdd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6232"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # Word Tokenization\n"
      ],
      "metadata": {
        "id": "vhrCq4YnI7Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2FH5CWpUaDR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize #Split a Sentence into Words\n",
        "\n",
        "tokens = {}\n",
        "\n",
        "for s in nltk_Sentences: #This goes through each sentence, then each word in that sentence.\n",
        "\n",
        "    sentence_tokens = word_tokenize(s)\n",
        "    for t in sentence_tokens:\n",
        "\n",
        "        if t not in tokens: #Couunts each words\n",
        "            tokens[t] = 0\n",
        "        tokens[t] += 1\n",
        "\n",
        "frequent_tokens = sorted(tokens, key=tokens.get, reverse=True)[:20] #This gives you the top 20 most common words in the book.\n",
        "for t in frequent_tokens:\n",
        "    print(t, \"\\t\\t\", tokens[t])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr31P4jyI-uh",
        "outputId": "664d38a9-28dc-4e77-cc76-964054958376"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", \t\t 5658\n",
            ". \t\t 5119\n",
            "the \t\t 3310\n",
            "'' \t\t 2441\n",
            "`` \t\t 2307\n",
            "to \t\t 1845\n",
            "and \t\t 1804\n",
            "a \t\t 1578\n",
            "Harry \t\t 1323\n",
            "was \t\t 1253\n",
            "of \t\t 1242\n",
            "he \t\t 1208\n",
            "'s \t\t 997\n",
            "in \t\t 933\n",
            "I \t\t 919\n",
            "it \t\t 897\n",
            "his \t\t 896\n",
            "you \t\t 837\n",
            "n't \t\t 826\n",
            "said \t\t 793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#N-Gram Computation\n",
        "\n",
        "ngrams: Generates combinations of words.  means bigrams (2-word pairs), e.g.:\n",
        "\n",
        "(\"Harry\", \"Potter\")"
      ],
      "metadata": {
        "id": "iU8jvHuELaJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams, word_tokenize\n",
        "\n",
        "nltk_sentence = nltk_Sentences[50]  # This selects the 51st sentence from your list of sentences\n",
        "\n",
        "sentence_tokens = word_tokenize(nltk_sentence) #This splits that sentence into individual words\n",
        "bigrams = list(ngrams(sentence_tokens, 2)) #This makes bigrams — pairs of two words.\n",
        "\n",
        "print(nltk_sentence)\n",
        "bigrams\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjSJ4oQMLMIT",
        "outputId": "8b40c90c-e441-4753-9c88-ac2002301ba6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He didn't know why, but they made him uneasy.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('He', 'did'),\n",
              " ('did', \"n't\"),\n",
              " (\"n't\", 'know'),\n",
              " ('know', 'why'),\n",
              " ('why', ','),\n",
              " (',', 'but'),\n",
              " ('but', 'they'),\n",
              " ('they', 'made'),\n",
              " ('made', 'him'),\n",
              " ('him', 'uneasy'),\n",
              " ('uneasy', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_sentence = spacy_sentences[100]  #Picks the 101st sentence from\n",
        "sentence_doc = nlp(spacy_sentence.text)\n",
        "\n",
        "import textacy\n",
        "ngrams = list(textacy.extract.basics.ngrams(sentence_doc, 2, filter_stops=False)) #Extracts bigrams using Textacy, even including stopwords (because filter_stops=False).\n",
        "\n",
        "print(spacy_sentence)\n",
        "ngrams\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THQUWG9gLeDz",
        "outputId": "8ea7c2f8-fa0b-44b8-c637-a68c29ebc055"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to be any more showers of owls tonight, Jim?\" \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Going to,\n",
              " to be,\n",
              " be any,\n",
              " any more,\n",
              " more showers,\n",
              " showers of,\n",
              " of owls,\n",
              " owls tonight]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POS Tagging (Part-of-Speech)"
      ],
      "metadata": {
        "id": "5nCP-FVGNo9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS labeling each word with its grammatical role, like noun, verb, adjective"
      ],
      "metadata": {
        "id": "Jy41Io8Dgixw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Sc2QcaNQJz",
        "outputId": "7f8ea144-c7cf-49ad-f89c-d047b85dc3b3"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print tokens each word with its tag WITH NLTK\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AU2WXpa1hZqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(sentence_tokens)\n",
        "for t, tag in pos_tags:\n",
        "    print(t, \"\\t\\t\", tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D1lgefvO3Lb",
        "outputId": "7bfa1ef3-603e-4cfd-85d1-26d90641bf50"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It \t\t PRP\n",
            "was \t\t VBD\n",
            "now \t\t RB\n",
            "sitting \t\t VBG\n",
            "on \t\t IN\n",
            "his \t\t PRP$\n",
            "garden \t\t NN\n",
            "wall \t\t NN\n",
            ". \t\t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging Using spaCy-  The POS tag from spaCy (like NOUN, VERB, ADJ"
      ],
      "metadata": {
        "id": "Sj3BbZIyihro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy_sentence)\n",
        "\n",
        "for t in sentence_doc:\n",
        "    print(t.text, \"\\t\\t\", t.pos_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNdAAkfwOVqi",
        "outputId": "0cfa6987-dd82-4e29-a353-e68ed1b29b36"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to be any more showers of owls tonight, Jim?\" \n",
            "\n",
            "\n",
            "Going \t\t VERB\n",
            "to \t\t PART\n",
            "be \t\t AUX\n",
            "any \t\t PRON\n",
            "more \t\t ADJ\n",
            "showers \t\t NOUN\n",
            "of \t\t ADP\n",
            "owls \t\t NOUN\n",
            "tonight \t\t NOUN\n",
            ", \t\t PUNCT\n",
            "Jim \t\t PROPN\n",
            "? \t\t PUNCT\n",
            "\" \t\t PUNCT\n",
            "\n",
            "\n",
            " \t\t SPACE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ُStremming"
      ],
      "metadata": {
        "id": "TRXh5Nl0PBF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduces each word to its stem/root but not accurate for example happily: happil!\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyb9AgziiwAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "print(nltk_sentence)\n",
        "\n",
        "porter = PorterStemmer()\n",
        "for t in sentence_tokens:\n",
        "    print(t, \"\\t\\t\", porter.stem(t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rykiUy8gPIAr",
        "outputId": "cfccbd5e-b11d-4789-ee76-c1a072e5f8d0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was now sitting on his garden wall.\n",
            "It \t\t it\n",
            "was \t\t wa\n",
            "now \t\t now\n",
            "sitting \t\t sit\n",
            "on \t\t on\n",
            "his \t\t hi\n",
            "garden \t\t garden\n",
            "wall \t\t wall\n",
            ". \t\t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ],
      "metadata": {
        "id": "nzEt2C-1PcVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same as stremmig but in  NLTK: Assumes words are nouns only! → results may be less accurate\n",
        "\n",
        "ensures word is root is valid.happlliy:happy."
      ],
      "metadata": {
        "id": "BUFop2DrjHVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(nltk_sentence)\n",
        "\n",
        "for t in sentence_tokens:\n",
        "    print(t, \"\\t\\t\", lemmatizer.lemmatize(t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bwUMxfcPnXq",
        "outputId": "20203c72-05a9-4766-ab45-e2d1234f8fcc"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was now sitting on his garden wall.\n",
            "It \t\t It\n",
            "was \t\t wa\n",
            "now \t\t now\n",
            "sitting \t\t sitting\n",
            "on \t\t on\n",
            "his \t\t his\n",
            "garden \t\t garden\n",
            "wall \t\t wall\n",
            ". \t\t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy understands the context and grammar → more accurate lemmatization"
      ],
      "metadata": {
        "id": "eBp0-hDzlIyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spacy_sentence)\n",
        "\n",
        "for t in sentence_doc:\n",
        "    print(t.text, \"\\t\\t\", t.lemma_ )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-W0k8mSPnFj",
        "outputId": "7aafeb27-5a34-489b-ba4a-51e39cd597f5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Going to be any more showers of owls tonight, Jim?\" \n",
            "\n",
            "\n",
            "Going \t\t go\n",
            "to \t\t to\n",
            "be \t\t be\n",
            "any \t\t any\n",
            "more \t\t more\n",
            "showers \t\t shower\n",
            "of \t\t of\n",
            "owls \t\t owl\n",
            "tonight \t\t tonight\n",
            ", \t\t ,\n",
            "Jim \t\t Jim\n",
            "? \t\t ?\n",
            "\" \t\t \"\n",
            "\n",
            "\n",
            " \t\t \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}